{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar todas as libs necessarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install pyyaml\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando o database csv para parquet\n",
    "\n",
    "\n",
    "Se ja tiver o arquivo em parquet, pular esta etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "csv_path = 'tce_fit.csv'\n",
    "output_dir = 'output_parquets'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_info):\n",
    "    chunk, index = chunk_info # chunk info , index ao qual essa chunk pertence\n",
    "    parquet_path = os.path.join(output_dir, f'tce_part_{index}.parquet')\n",
    "    chunk.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "    return f\"Processed part {index} saved to {parquet_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed part 0 saved to output_parquets/tce_part_0.parquet\n",
      "Processed part 1 saved to output_parquets/tce_part_1.parquet\n",
      "Processed part 2 saved to output_parquets/tce_part_2.parquet\n",
      "Processed part 3 saved to output_parquets/tce_part_3.parquet\n",
      "Processed part 4 saved to output_parquets/tce_part_4.parquet\n",
      "Processed part 5 saved to output_parquets/tce_part_5.parquet\n",
      "Processed part 6 saved to output_parquets/tce_part_6.parquet\n",
      "Processed part 7 saved to output_parquets/tce_part_7.parquet\n",
      "Processed part 8 saved to output_parquets/tce_part_8.parquet\n",
      "Processed part 9 saved to output_parquets/tce_part_9.parquet\n"
     ]
    }
   ],
   "source": [
    "# Total rows excluding header\n",
    "total_rows = sum(1 for _ in open(csv_path)) - 1\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_size = total_rows // 10 + 1  # Determine chunk size for 10 parts\n",
    "\n",
    "# Specify dtype to avoid conversion errors\n",
    "chunks = []\n",
    "for index, chunk in enumerate(pd.read_csv(\n",
    "    csv_path,\n",
    "    chunksize=chunk_size,\n",
    "    sep=';',\n",
    "    on_bad_lines='skip',\n",
    "    dtype={'CPFCNPJCredor': 'object'},  # Ensure this column is read as string\n",
    "    low_memory=False,  # Prevent pandas from reading in smaller parts and inferring types\n",
    "    )):\n",
    "    chunks.append((chunk, index))\n",
    "\n",
    "try: \n",
    "    # Use multiprocessing to process each chunk in parallel\n",
    "    with Pool(processes=10) as pool:\n",
    "        results = pool.map(process_chunk, chunks) # The Pool.map() function ensures the chunks are processed in the \n",
    "                                                # same order as they are provided in the input (chunks list).                            \n",
    "    for result in results:\n",
    "        print(result)                                            \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading output_parquets/tce_part_0.parquet\n",
      "Reading output_parquets/tce_part_1.parquet\n",
      "Reading output_parquets/tce_part_2.parquet\n",
      "Reading output_parquets/tce_part_3.parquet\n",
      "Reading output_parquets/tce_part_4.parquet\n",
      "Reading output_parquets/tce_part_5.parquet\n",
      "Reading output_parquets/tce_part_6.parquet\n",
      "Reading output_parquets/tce_part_7.parquet\n",
      "Reading output_parquets/tce_part_8.parquet\n",
      "Reading output_parquets/tce_part_9.parquet\n",
      "Reassembled Parquet file saved to reassembled_tce.parquet\n"
     ]
    }
   ],
   "source": [
    "# Reassembling the parquet files into a single one\n",
    "output_file = 'reassembled_tce.parquet'\n",
    "\n",
    "# List all Parquet files in the directory and sort them in ascending order\n",
    "parquet_files = sorted([f for f in os.listdir(output_dir) if f.startswith('tce_part_') and f.endswith('.parquet')])\n",
    "#print(\"parquet files \", parquet_files)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each Parquet file in order and append to the list\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    print(f\"Reading {file_path}\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a single Parquet file\n",
    "final_df.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "print(f\"Reassembled Parquet file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database pronto para ser processado.\n",
    "Agora iremos pegar os valores que nos interessam.\n",
    "Colunas: 'unidade', 'elemDespesaTCE', 'histórico' e 'idcontrato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_model': 'sentence-transformers/all-MiniLM-L12-v1', 'embedding_batch_size': 128}\n"
     ]
    }
   ],
   "source": [
    "# Open the configuration file and load the different arguments\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from a Parquet file\n",
    "df = pd.read_parquet('reassembled_tce.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Analise', 'IdEmpenho', 'Ano', 'Vlr_AnulacaoEmpenho',\n",
      "       'CdFonteTCE', 'CdFonteUG', 'CNPJRaiz', 'CPFCNPJCredorQtNrs',\n",
      "       'CPFCNPJCredor', 'Credor', 'DtEmpenho', 'DEFonteTCE', 'DEFonteUG',\n",
      "       'DEPrograma', 'DEProjAtiv', 'DtAnomes', 'Elemento', 'ElemDespesaTCE',\n",
      "       'ElemDespesaUG', 'Ente', 'Esfera', 'Funcao', 'Historico', 'IdContrato',\n",
      "       'IdFonte', 'IdFuncao', 'Id_Orgao', 'IdPrograma', 'IdSubFuncao',\n",
      "       'IdUnid', 'IdOrgao', 'NrFonte', 'NrFonteUG', 'NrLicitacao',\n",
      "       'NrProjAtiv', 'NrEmpenho', 'ProgTrab', 'ProgTrabRed', 'ProjAtiv',\n",
      "       'SubFuncao', 'Tp_Empenho', 'Unidade', 'Vlr_Empenho',\n",
      "       'Vlr_Anul_Liquidacao', 'Vlr_Liquidacao', 'Vlr_Pagto', 'Vlr_Retencao',\n",
      "       'Vlr_SubEmpenho', 'Vlr_Empenhado', 'Vlr_Liquidado', 'Vlr_Pago',\n",
      "       'CGElem', 'CGProgTrab', 'CGigual', 'Cod_Elem', 'Cod_PT', 'CG',\n",
      "       'CGtitulo', 'CGDesc', 'CGtitTCE', 'CGfreq', 'CGlevel', 'CGpai',\n",
      "       'CGroot', 'CGchild'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# columns\n",
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho total da base:  1484918\n"
     ]
    }
   ],
   "source": [
    "# consultar o tamanho total da base\n",
    "print(\"tamanho total da base: \", len(df.iloc[:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos carregar o modelo de embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# truncate_dim=256\n",
    "model = SentenceTransformer(f'{config['embedding_model']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "historico_teste = df['Historico'][0:384].astype(str).tolist()  # Ensure it's a list of strings\n",
    "\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return model.encode(batch)\n",
    "\n",
    "# Split data into batches\n",
    "batch_size = 128\n",
    "batches = [historico_teste[i:i+batch_size] for i in range(0, len(historico_teste), batch_size)]\n",
    "\n",
    "# Parallel processing with threads\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust max_workers as needed\n",
    "    for i, result in enumerate(tqdm(executor.map(encode_batch, batches), total=len(batches))):\n",
    "        # Convert the result to a Torch tensor\n",
    "        tensor_embeddings = torch.tensor(result)\n",
    "        \n",
    "        # Save the tensor as a .pt file\n",
    "        torch.save(tensor_embeddings, os.path.join(f\"output_embeddings/embeddings_batch_{i}.pt\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinicius.goncalvez-mfvx-2cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
