{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar todas as libs necessarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install pyyaml\n",
    "!pip install -U sentence-transformers\n",
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_model': 'sentence-transformers/all-MiniLM-L12-v1', 'embedding_batch_size': 128, 'threshold_reducing_units': 95}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import yaml\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Open the configuration file and load the different arguments\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando o database csv para parquet\n",
    "\n",
    "\n",
    "Se ja tiver o arquivo em parquet, pular esta etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "csv_path = 'tce_fit.csv'\n",
    "output_dir = 'output_parquets'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_info):\n",
    "    chunk, index = chunk_info # chunk info , index ao qual essa chunk pertence\n",
    "    parquet_path = os.path.join(output_dir, f'tce_part_{index}.parquet')\n",
    "    chunk.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "    return f\"Processed part {index} saved to {parquet_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed part 0 saved to output_parquets/tce_part_0.parquet\n",
      "Processed part 1 saved to output_parquets/tce_part_1.parquet\n",
      "Processed part 2 saved to output_parquets/tce_part_2.parquet\n",
      "Processed part 3 saved to output_parquets/tce_part_3.parquet\n",
      "Processed part 4 saved to output_parquets/tce_part_4.parquet\n",
      "Processed part 5 saved to output_parquets/tce_part_5.parquet\n",
      "Processed part 6 saved to output_parquets/tce_part_6.parquet\n",
      "Processed part 7 saved to output_parquets/tce_part_7.parquet\n",
      "Processed part 8 saved to output_parquets/tce_part_8.parquet\n",
      "Processed part 9 saved to output_parquets/tce_part_9.parquet\n"
     ]
    }
   ],
   "source": [
    "# Total rows excluding header\n",
    "total_rows = sum(1 for _ in open(csv_path)) - 1\n",
    "\n",
    "# Read the CSV file in chunks\n",
    "chunk_size = total_rows // 10 + 1  # Determine chunk size for 10 parts\n",
    "\n",
    "# Specify dtype to avoid conversion errors\n",
    "chunks = []\n",
    "for index, chunk in enumerate(pd.read_csv(\n",
    "    csv_path,\n",
    "    chunksize=chunk_size,\n",
    "    sep=';',\n",
    "    on_bad_lines='skip',\n",
    "    dtype={'CPFCNPJCredor': 'object'},  # Ensure this column is read as string\n",
    "    low_memory=False,  # Prevent pandas from reading in smaller parts and inferring types\n",
    "    )):\n",
    "    chunks.append((chunk, index))\n",
    "\n",
    "try: \n",
    "    # Use multiprocessing to process each chunk in parallel\n",
    "    with Pool(processes=10) as pool:\n",
    "        results = pool.map(process_chunk, chunks) # The Pool.map() function ensures the chunks are processed in the \n",
    "                                                # same order as they are provided in the input (chunks list).                            \n",
    "    for result in results:\n",
    "        print(result)                                            \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading output_parquets/tce_part_0.parquet\n",
      "Reading output_parquets/tce_part_1.parquet\n",
      "Reading output_parquets/tce_part_2.parquet\n",
      "Reading output_parquets/tce_part_3.parquet\n",
      "Reading output_parquets/tce_part_4.parquet\n",
      "Reading output_parquets/tce_part_5.parquet\n",
      "Reading output_parquets/tce_part_6.parquet\n",
      "Reading output_parquets/tce_part_7.parquet\n",
      "Reading output_parquets/tce_part_8.parquet\n",
      "Reading output_parquets/tce_part_9.parquet\n",
      "Reassembled Parquet file saved to reassembled_tce.parquet\n"
     ]
    }
   ],
   "source": [
    "# Reassembling the parquet files into a single one\n",
    "output_file = 'reassembled_tce.parquet'\n",
    "\n",
    "# List all Parquet files in the directory and sort them in ascending order\n",
    "parquet_files = sorted([f for f in os.listdir(output_dir) if f.startswith('tce_part_') and f.endswith('.parquet')])\n",
    "#print(\"parquet files \", parquet_files)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each Parquet file in order and append to the list\n",
    "for file in parquet_files:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    print(f\"Reading {file_path}\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a single Parquet file\n",
    "final_df.to_parquet(output_file, engine='pyarrow', index=False)\n",
    "print(f\"Reassembled Parquet file saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database pronto para ser processado.**\n",
    "\n",
    "\n",
    "Agora iremos pegar os valores que nos interessam.\n",
    "Colunas: 'unidade', 'elemDespesaTCE', 'hist√≥rico' e 'idcontrato'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame from a Parquet file\n",
    "df = pd.read_parquet('reassembled_tce.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Analise', 'IdEmpenho', 'Ano', 'Vlr_AnulacaoEmpenho',\n",
      "       'CdFonteTCE', 'CdFonteUG', 'CNPJRaiz', 'CPFCNPJCredorQtNrs',\n",
      "       'CPFCNPJCredor', 'Credor', 'DtEmpenho', 'DEFonteTCE', 'DEFonteUG',\n",
      "       'DEPrograma', 'DEProjAtiv', 'DtAnomes', 'Elemento', 'ElemDespesaTCE',\n",
      "       'ElemDespesaUG', 'Ente', 'Esfera', 'Funcao', 'Historico', 'IdContrato',\n",
      "       'IdFonte', 'IdFuncao', 'Id_Orgao', 'IdPrograma', 'IdSubFuncao',\n",
      "       'IdUnid', 'IdOrgao', 'NrFonte', 'NrFonteUG', 'NrLicitacao',\n",
      "       'NrProjAtiv', 'NrEmpenho', 'ProgTrab', 'ProgTrabRed', 'ProjAtiv',\n",
      "       'SubFuncao', 'Tp_Empenho', 'Unidade', 'Vlr_Empenho',\n",
      "       'Vlr_Anul_Liquidacao', 'Vlr_Liquidacao', 'Vlr_Pagto', 'Vlr_Retencao',\n",
      "       'Vlr_SubEmpenho', 'Vlr_Empenhado', 'Vlr_Liquidado', 'Vlr_Pago',\n",
      "       'CGElem', 'CGProgTrab', 'CGigual', 'Cod_Elem', 'Cod_PT', 'CG',\n",
      "       'CGtitulo', 'CGDesc', 'CGtitTCE', 'CGfreq', 'CGlevel', 'CGpai',\n",
      "       'CGroot', 'CGchild'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# columns\n",
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanho total da base:  1484918\n"
     ]
    }
   ],
   "source": [
    "# consultar o tamanho total da base\n",
    "print(\"tamanho total da base: \", len(df.iloc[:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos carregar o modelo de embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinicius.goncalvez/.local/share/virtualenvs/vinicius.goncalvez-mfvx-2cv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# truncate_dim=256\n",
    "model = SentenceTransformer(f'{config['embedding_model']}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historico = df['Historico'].astype(str).tolist()  # Ensure it's a list of strings\n",
    "\n",
    "\n",
    "def encode_batch(batch):\n",
    "    return model.encode(batch)\n",
    "\n",
    "# Split data into batches\n",
    "batch_size = 128\n",
    "batches = [historico[i:i+batch_size] for i in range(0, len(historico), batch_size)]\n",
    "\n",
    "# Parallel processing with threads\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Adjust max_workers as needed\n",
    "    for i, result in enumerate(tqdm(executor.map(encode_batch, batches), total=len(batches))):\n",
    "        # Convert the result to a Torch tensor\n",
    "        tensor_embeddings = torch.tensor(result)\n",
    "        \n",
    "        # Save the tensor as a .pt file\n",
    "        torch.save(tensor_embeddings, os.path.join(f\"output_embeddings/embeddings_batch_{i}.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pr√©-processando os outros dados a serem utilizados na clusteriza√ßao**\n",
    "\n",
    "Colunas com dados categoricos, como Unidade e ElemDespesaTCE, podem ser transformados em formato one_hot_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000363087/2020\n",
      "PREFEITURA ANGRA DOS REIS\n",
      "OUTROS SERVICOS DE TERCEIROS   PESSOA JURIDICA\n"
     ]
    }
   ],
   "source": [
    "idcontrato = df['IdContrato']\n",
    "unidades = df['Unidade']\n",
    "elemdespesatce = df['ElemDespesaTCE']\n",
    "\n",
    "print(idcontrato[0])\n",
    "print(unidades[0])\n",
    "print(elemdespesatce[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories after grouping: 757\n",
      "\n",
      "Original: FUNDO MUN SAUDE BARRA DO PIRAI -> Grouped: FUNDO MUN SAUDE PIRAI\n",
      "Original: FUNDO MUN ASSIST SOCIAL PIRAI -> Grouped: FUNDO MUN ASSIST SOCIAL BARRA DO PIRAI\n",
      "Original: FUNDO MUN ASSIST SOCIAL TANGUA -> Grouped: FUNDO MUN ASSIST SOCIAL ITAGUAI\n",
      "Original: FUNDO MUN  DE ASSISTENCIA SOCIAL -> Grouped: FUNDO MUN DE ASSISTENCIA SOCIAL MARICA\n",
      "Original: FUNDO MUN ASSISTENCIA SOCIAL DE ITATIAIA -> Grouped: FUNDO MUN  DE ASSISTENCIA SOCIAL\n",
      "Original: FUNDO MUN  MEIO AMBIENTE -> Grouped: FUNDO MUN MEIO AMBIENTE DE ANGRA\n",
      "Original: FUNDO MUN  DE DES  DO MEIO AMBIENTE -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN DEFESA DO MEIO AMBIENTE -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN DE MEIO AMBIENTE -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN MEIO AMBIENTE CABO FRIO -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN MEIO AMBIENTE SUMIDOURO -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN  DO MEIO AMBIENTE -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN MEIO AMBIENTE BARRA MANSA -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN  DO MEIO AMBIENTE DE JAPERI -> Grouped: FUNDO MUN DE MEIO AMBIENTE\n",
      "Original: FUNDO MUN MEIO AMBIENTE APERIBE -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO MUN MEIO AMBIENTE SAQUAREMA -> Grouped: FUNDO MUN  MEIO AMBIENTE\n",
      "Original: FUNDO ESP CAMARA BOM JESUS DO ITABAPOANA -> Grouped: CAMARA BOM JESUS DO ITABAPOANA\n"
     ]
    }
   ],
   "source": [
    "from processing_utils import group_similar_categories\n",
    "# Apply the grouping function to the 'unidades' column\n",
    "unidades_grouped, mapping = group_similar_categories(unidades, threshold=config['threshold_reducing_units'])\n",
    "\n",
    "# Check the reduced categories\n",
    "print(f\"Number of unique categories after grouping: {unidades_grouped.nunique()}\\n\")\n",
    "\n",
    "# Filter the mapping to show only changed values\n",
    "changed_mappings = {original: grouped for original, grouped in mapping.items() if original != grouped}\n",
    "\n",
    "# Print the changed mappings\n",
    "for original, grouped in changed_mappings.items():\n",
    "    print(f\"Original: {original} -> Grouped: {grouped}\")\n",
    "\n",
    "# Save to a file for easier inspection (optional)\n",
    "changed_mapping_df = pd.DataFrame(list(changed_mappings.items()), columns=[\"Original\", \"Grouped\"])\n",
    "changed_mapping_df.to_csv(\"changed_unidades_grouped_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez as unidades reduzidas, podemos categorizar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['A CLASSIFICAR', 'A DEFINIR',\n",
      "       'ALIQUOTA SUPLEMENTAR DE CONTRIBUICAO PREVIDENCIARIA   PESSOAL ATIVO   PLANO FINANCEIRO',\n",
      "       'APLICACAO DIRETA A CONTA DE RECURSOS DE QUE TRATA O ART  25 DA LEI COMPLEMENTAR NO 141  DE 2012 ',\n",
      "       'APLICACAO DIRETA A CONTA DE RECURSOS DE QUE TRATAM OS 1O E 2O DO ART  24 DA LEI COMPLEMENTAR NO 141  DE 2012 ',\n",
      "       'APLICACAO DIRETA DECORRENTE DE OPERACAO ENTRE ORGAOS  FUNDOS E ENTIDADES INTEGRANTES DOS ORCAMENTOS FISCAL E DA SEGURIDADE SOCIAL',\n",
      "       'APLICACOES DIRETAS',\n",
      "       'APORTE PARA COBERTURA DO DEFICIT ATUARIAL DO RPPS',\n",
      "       'APOSENTADORIAS DO RPPS  RESERVA REMUNERADA E REFORMAS DOS MILITARES',\n",
      "       'AQUISICAO DE IMOVEIS',\n",
      "       ...\n",
      "       'TRANSFERENCIAS A INSTITUICOES MULTIGOVERNAMENTAIS',\n",
      "       'TRANSFERENCIAS A INSTITUICOES PRIVADAS COM FINS LUCRATIVOS',\n",
      "       'TRANSFERENCIAS A INSTITUICOES PRIVADAS SEM FINS LUCRATIVOS',\n",
      "       'TRANSFERENCIAS A MUNICIPIOS', 'TRANSFERENCIAS A UNIAO',\n",
      "       'TRANSFERENCIAS FUNDO A FUNDO AOS ESTADOS E AO DISTRITO FEDERAL A CONTA DE RECURSOS DE QUE TRATA O ART  25 DA LEI COMPLEMENTAR NO 141  DE 2012 ',\n",
      "       'TRANSFERENCIAS FUNDO A FUNDO AOS MUNICIPIOS A CONTA DE RECURSOS DE QUE TRATAM OS 1O E 2O DO ART  24 DA LEI COMPLEMENTAR NO 141  DE 2012 ',\n",
      "       'VENCIMENTOS E VANTAGENS FIXAS   PESSOAL CIVIL',\n",
      "       'VENCIMENTOS E VANTAGENS FIXAS  EXCETO SALARIO MATERNIDADE E LICENCA SAUDE  PESSOAL CIVIL',\n",
      "       'VENCIMENTOS E VANTAGENS FIXAS PESSOAL CIVIL'],\n",
      "      dtype='object', length=129)\n"
     ]
    }
   ],
   "source": [
    "from processing_utils import transformDataInCategory\n",
    "\n",
    "# unidades_one_hot is going to return a table with 1484918 rows and 771 columns\n",
    "# there are 771 unidades, so for each row, it's going to return true for its corresponding unidade\n",
    "# and false for all the others\n",
    "unidades_one_hot = transformDataInCategory(unidades)\n",
    "\n",
    "# same thing goes for the elemdespesatce column\n",
    "elemdespesatce_one_hot = transformDataInCategory(elemdespesatce)\n",
    "\n",
    "print(elemdespesatce_one_hot.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting ano and idcontrato from the idcontrato column\n",
    "idcontrato_numero = idcontrato.str.split('/').str[0]  # N√∫mero do contrato\n",
    "idcontrato_ano = idcontrato.str.split('/').str[1]     # Ano do contrato"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vinicius.goncalvez-mfvx-2cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
